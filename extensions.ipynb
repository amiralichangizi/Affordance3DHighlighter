{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30823,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "#git clone with your token\n!git clone --branch extensions https://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@github.com/amiralichangizi/Affordance3DHighlighter.git",
   "metadata": {
    "id": "2rqJobuCpQLi",
    "outputId": "67a93806-440e-4b7c-a56e-c43debd5ced5",
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "!git pull origin extensions",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import os\n\nos.chdir('/kaggle/working/Affordance3DHighlighter')",
   "metadata": {
    "id": "fhLYzi952EEA",
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "!pip install gdown\n!gdown --id 1siZtGusB1LfQVapTvNOiYi8aeKKAgcDF\n!unzip full-shape.zip -d /kaggle/working/Affordance3DHighlighter/data/",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import pickle\nfrom collections import defaultdict\n\n# Load training data\nwith open('/kaggle/working/Affordance3DHighlighter/data/full_shape_train_data.pkl', 'rb') as train_file:\n    dataset = pickle.load(train_file)\n\n# Create a set to store unique semantic classes\nsemantic_classes = set()\nall_affordances = dataset[0]['affordance']\n\n# Iterate through the dataset and collect semantic classes\nfor item in dataset:\n    semantic_class = item['semantic class']\n    semantic_classes.add(semantic_class)\n\n# Print all unique semantic classes\nprint(\"Unique semantic classes in the dataset:\")\nfor cls in sorted(semantic_classes):\n    print(f\"- {cls}\")\n\n# Print all unique semantic classes\nprint(\"Unique Affordances in the dataset:\")\nfor cls in sorted(all_affordances):\n    print(f\"- {cls}\")\n\n# Print total count\nprint(f\"\\nTotal number of unique semantic classes: {len(semantic_classes)}\")\nprint(f\"\\nTotal number of unique Affordances: {len(all_affordances)}\")\n\n# Optional: Print count of items per semantic class\nclass_counts = {}\nfor item in dataset:\n    semantic_class = item['semantic class']\n    class_counts[semantic_class] = class_counts.get(semantic_class, 0) + 1\n\nprint(\"\\nNumber of items per semantic class:\")\nfor cls, count in sorted(class_counts.items()):\n    print(f\"- {cls}: {count} items\")\n",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "!pip install git+https://github.com/openai/CLIP.git\n!pip install kaolin==0.17.0 -f https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.5.1_cu121.html",
   "metadata": {
    "id": "d8vCbctxbPP4",
    "outputId": "303a2d90-d6d8-4bbd-97fa-621867d8c52c",
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "\nimport sys\nimport torch\n\nneed_pytorch3d = False\ntry:\n    import pytorch3d\nexcept ModuleNotFoundError:\n    need_pytorch3d = True\nif need_pytorch3d:\n    pyt_version_str = torch.__version__.split(\"+\")[0].replace(\".\", \"\")\n    version_str = \"\".join([\n        f\"py3{sys.version_info.minor}_cu\",\n        torch.version.cuda.replace(\".\", \"\"),\n        f\"_pyt{pyt_version_str}\"\n    ])\n    !pip install iopath\n    if sys.platform.startswith(\"linux\"):\n        print(\"Trying to install wheel for PyTorch3D\")\n        !pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html\n        pip_list = !pip freeze\n        need_pytorch3d = not any(i.startswith(\"pytorch3d==\") for i in pip_list)\n    if need_pytorch3d:\n        print(f\"failed to find/install wheel for {version_str}\")\nif need_pytorch3d:\n    print(\"Installing PyTorch3D from source\")\n    !pip install ninja\n    !pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T14:58:53.585274Z",
     "start_time": "2024-12-26T14:58:48.836644Z"
    },
    "id": "sRNWfRMnIzuJ",
    "outputId": "cbebc8f4-6572-4a67-a33e-060c1d89ce4e",
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "!pip install --ignore-installed open3d",
   "metadata": {
    "id": "zT9LfpcRXDHy",
    "outputId": "a506d6d8-8390-403f-ac0b-c079ebdf6aee",
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "\nfrom src.mesh import Mesh\nfrom pytorch3d.structures import Pointclouds\n\nfrom src.convertor import obj_to_pointcloud\n\n\ndef bounding_sphere_normalize(points: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    points: (N,3) tensor of point coords\n    Return normalized points in a unit sphere centered at origin.\n    \"\"\"\n    center = points.mean(dim=0, keepdim=True)\n    max_dist = (points - center).norm(p=2, dim=1).max()\n    points_normed = (points - center) / max_dist\n    return points_normed\n\n\ndef load_3d_data(file_path, num_points=10000, device=\"cuda\", do_normalize=True):\n    \"\"\"\n    Loads 3D data as PyTorch3D Pointclouds from either NPZ point cloud or OBJ mesh.\n\n    Args:\n        file_path: Path to either .npz point cloud or .obj mesh file\n        num_points: Number of points to sample if loading from mesh\n        device: Device to load data on\n\n    Returns:\n        Pointclouds object containing points and features\n    \"\"\"\n    file_ext = file_path.split('.')[-1].lower()\n\n    if file_ext == 'npz':\n        # Load NPZ point cloud directly like in the example\n        pointcloud = np.load(file_path)\n        verts = torch.Tensor(pointcloud['verts']).to(device)\n        rgb = torch.Tensor(pointcloud['rgb']).to(device)\n\n        print(\"lenght of the data\")\n        print(len(verts))\n\n        # Subsample if needed\n        if len(verts) > num_points:\n            idx = torch.randperm(len(verts))[:num_points]\n            verts = verts[idx]\n            rgb = rgb[idx]\n\n        if do_normalize:\n            verts = bounding_sphere_normalize(verts)\n\n        # Return both the points tensor and the Pointclouds object\n        point_cloud = Pointclouds(points=[verts], features=[rgb])\n        return verts, point_cloud  # Return both\n\n    elif file_ext == 'obj':\n        # Load and convert your OBJ file\n        points, point_cloud = obj_to_pointcloud(\n            file_path,\n            num_points=num_points,  # Adjust this number as needed\n            device=\"cuda\"  # Use \"cpu\" if you don't have a GPU\n        )\n        if do_normalize:\n            points = bounding_sphere_normalize(points)\n            # here we update the point cloud too\n            rgb = point_cloud.features_packed()  # shape [N,3]\n            point_cloud = Pointclouds(points=[points], features=[rgb])\n        return points, point_cloud\n\n    else:\n        raise ValueError(f\"Unsupported file format: {file_ext}. Only .npz and .obj are supported.\")\n\n",
   "metadata": {
    "id": "fxo1SjSH2NHm",
    "outputId": "fd9c90bb-1f98-4e61-ee27-b43b8f1bbd64",
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def save_experiment_report(background_info, shape_results, global_miou, output_dir):\n",
    "    \"\"\"\n",
    "    Saves experiment results to a report file.\n",
    "    \n",
    "    Args:\n",
    "        background_info (str): Description of background used\n",
    "        shape_results (list): List of dictionaries containing per-shape results\n",
    "        global_miou (float): Overall mIoU across all shapes\n",
    "        output_dir (str): Directory to save the report\n",
    "    \"\"\"\n",
    "    import json\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Create report directory if it doesn't exist\n",
    "    report_dir = os.path.join(output_dir, \"reports\")\n",
    "    os.makedirs(report_dir, exist_ok=True)\n",
    "    \n",
    "    # Generate timestamp for unique filename\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    report = {\n",
    "        \"timestamp\": timestamp,\n",
    "        \"background\": background_info,\n",
    "        \"global_miou\": float(global_miou),\n",
    "        \"shape_results\": shape_results,\n",
    "    }\n",
    "    \n",
    "    # Save as JSON file\n",
    "    report_path = os.path.join(report_dir, f\"report_{timestamp}.json\")\n",
    "    with open(report_path, 'w') as f:\n",
    "        json.dump(report, f, indent=4)\n",
    "        \n",
    "    # Also save a human-readable summary\n",
    "    summary_path = os.path.join(report_dir, f\"summary_{timestamp}.txt\")\n",
    "    with open(summary_path, 'w') as f:\n",
    "        f.write(f\"Experiment Report\\n\")\n",
    "        f.write(f\"================\\n\")\n",
    "        f.write(f\"Date: {timestamp}\\n\")\n",
    "        f.write(f\"Background: {background_info}\\n\")\n",
    "        f.write(f\"Global mIoU: {global_miou:.4f}\\n\\n\")\n",
    "        f.write(\"Per-Shape Results:\\n\")\n",
    "        for result in shape_results:\n",
    "            f.write(f\"\\nShape {result['shape_class']} (ID: {result['shape_id']}):\\n\")\n",
    "            f.write(f\"  Shape mIoU: {result['shape_miou']:.4f}\\n\")\n",
    "            f.write(\"  Per-Affordance IoU:\\n\")\n",
    "            for aff, iou in result['affordance_ious'].items():\n",
    "                f.write(f\"    - {aff}: {iou:.4f}\\n\")"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-01-10T20:04:33.103212Z",
     "iopub.execute_input": "2025-01-10T20:04:33.103667Z",
     "iopub.status.idle": "2025-01-10T20:04:33.113244Z",
     "shell.execute_reply.started": "2025-01-10T20:04:33.103628Z",
     "shell.execute_reply": "2025-01-10T20:04:33.112401Z"
    }
   },
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "source": [
    "from src.utils import compute_mIoU\n",
    "from src.prompt_strategies import generate_affordance_prompt\n",
    "from src.data_loader_fullshape import FullShapeDataset\n",
    "from src.render.cloud_point_renderer import MultiViewPointCloudRenderer\n",
    "from src.save_results import save_renders, save_results\n",
    "from src.neural_highlighter import NeuralHighlighter\n",
    "from src.Clip.loss_function import clip_loss\n",
    "from src.Clip.clip_model import get_clip_model, encode_text, setup_clip_transforms\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Constrain most sources of randomness\n",
    "# (some torch backwards functions within CLIP are non-determinstic)\n",
    "# Set a consistent seed for reproducibility\n",
    "seed = 0  # You can use any integer value\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def optimize_point_cloud(points, clip_model,clip_transform, augment_transform, renderer, encoded_text, log_dir: str,background_path=None, **kwargs):\n",
    "    num_iterations = kwargs.get('num_iterations', 1000)\n",
    "    learning_rate = kwargs.get('learning_rate', 1e-4)\n",
    "    depth = kwargs.get('depth', 5)\n",
    "    width = kwargs.get('network_width', 256)\n",
    "    n_views = kwargs.get(\"n_views\", 4)\n",
    "    n_augs = kwargs.get('n_augs', 1)\n",
    "    clipavg = kwargs.get('clipavg', 'view')\n",
    "    device = kwargs.get('device', 'cuda')\n",
    "    \n",
    "\n",
    "    # Initialize network and optimizer\n",
    "    net = NeuralHighlighter(\n",
    "        depth=depth,  # Number of hidden layers\n",
    "        width=width,  # Width of each layer\n",
    "        out_dim=2,  # Binary classification (highlight/no-highlight)\n",
    "        input_dim=3,  # 3D coordinates (x,y,z)\n",
    "        positional_encoding=False  # As recommended in the paper\n",
    "    ).to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    for i in tqdm(range(num_iterations)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Predict highlight probabilities\n",
    "        pred_class = net(points)\n",
    "\n",
    "        # Create colors based on predictions\n",
    "        highlight_color = torch.tensor([204 / 255, 1.0, 0.0]).to(device)\n",
    "        base_color = torch.tensor([180 / 255, 180 / 255, 180 / 255]).to(device)\n",
    "\n",
    "        colors = pred_class[:, 0:1] * highlight_color + pred_class[:, 1:2] * base_color\n",
    "\n",
    "        # Create and render point cloud\n",
    "        point_cloud = renderer.create_point_cloud(points, colors)\n",
    "        rendered_images = renderer.render_all_views(\n",
    "            point_cloud=point_cloud,\n",
    "            n_views=n_views,\n",
    "            background_path=background_path\n",
    "        )\n",
    "        # Convert dictionary of images to tensor\n",
    "        rendered_tensor = []\n",
    "        for name, img in rendered_images.items():\n",
    "            rendered_tensor.append(img.to(device))\n",
    "        rendered_tensor = torch.stack(rendered_tensor)\n",
    "\n",
    "        #Convert rendered images to CLIP format\n",
    "        rendered_images = rendered_tensor.permute(0, 3, 1, 2)  # [B, H, W, C] -> [B, C, H, W]\n",
    "        #print(rendered_images.shape)\n",
    "\n",
    "        # Calculate CLIP loss\n",
    "        loss = clip_loss(\n",
    "            rendered_images=rendered_images,\n",
    "            encoded_text=encoded_text,\n",
    "            clip_transform=clip_transform,\n",
    "            augment_transform=augment_transform,\n",
    "            clip_model=clip_model,\n",
    "            n_augs=n_augs,\n",
    "            clipavg=clipavg\n",
    "        )\n",
    "        #print(\"Loss computation graph:\")\n",
    "        #print_grad_fn(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}, Loss: {loss.item():.4f}\")\n",
    "            save_renders(log_dir, i, rendered_images)\n",
    "\n",
    "    return net\n",
    "\n",
    "\n",
    "def main(input_path,target_classes=['Bowl'],target_affordances=['contain'],prompt_strategy=\"affordance_specific\",iou_threshold = 0.15, **kwargs):\n",
    "    \"\"\"\n",
    "    Main function for 3D highlighting with configurable parameters.\n",
    "\n",
    "    Args:\n",
    "        input_path: Path to input 3D file (mesh or point cloud)\n",
    "        object_name: Name of the object for the prompt\n",
    "        highlight_region: Region to highlight\n",
    "        **kwargs: Optional parameters with defaults:\n",
    "            n_views: Number of views to render (default: 5)\n",
    "            n_aug: Number of augmentations (default: 5)\n",
    "            clipavg: Method for CLIP averaging (default: \"view\")\n",
    "            network_depth: Depth of neural network (default: 5)\n",
    "            network_width: Width of neural layers (default: 256)\n",
    "            learning_rate: Learning rate for optimization (default: 1e-4)\n",
    "            num_iterations: Number of training iterations (default: 500)\n",
    "            num_points: Number of points to sample (default: 10000)\n",
    "            device: Device to run on (default: \"cuda\")\n",
    "            output_dir: Directory for outputs (default: \"./output\")\n",
    "    \"\"\"\n",
    "    # Extract parameters from kwargs with defaults\n",
    "    n_views = kwargs.get(\"n_views\", 4)\n",
    "    num_points = kwargs.get(\"num_points\", 10000)\n",
    "    device = kwargs.get(\"device\", \"cuda\")\n",
    "    output_dir = kwargs.get(\"output_dir\", \"./output\")\n",
    "    do_normalize = kwargs.get(\"do_normalize\", True)\n",
    "    background_paths = kwargs.get(\"background_paths\", [None])\n",
    "\n",
    "    try:\n",
    "\n",
    "        # LOAD AffordanceNet Dataset\n",
    "        file_type = input_path.split(\".\")[-1]\n",
    "        print(f\"Loading AffordanceNet Dataset...\")\n",
    "        if file_type == \"pkl\":\n",
    "            dataset = FullShapeDataset(\n",
    "                input_path,\n",
    "                target_classes=target_classes,\n",
    "                target_affordances=target_affordances,\n",
    "                device=device\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid file format. Expected .pkl file,got: {file_type}\")\n",
    "\n",
    "        val_indices = list(range(min(3, len(dataset))))\n",
    "\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Setup CLIP model\n",
    "        print(\"Setting up CLIP model...\")\n",
    "        clip_model, preprocess, resolution = get_clip_model()\n",
    "\n",
    "        # Initialize renderer\n",
    "        print(\"Setting up renderer...\")\n",
    "        renderer = MultiViewPointCloudRenderer(\n",
    "            camera_type=\"perspective\",\n",
    "            image_size=512,\n",
    "            base_dist=2.5,  # Your default view distance\n",
    "            base_elev=10,  # Your default elevation\n",
    "            base_azim=45,  # Your default azimuth\n",
    "            device=device,\n",
    "            point_radius=0.008\n",
    "        )\n",
    "\n",
    "        transforms_list = ['default','balanced','viewpoint','lighting']\n",
    "\n",
    "        for aug_type in transforms_list:\n",
    "\n",
    "            print(50*\"=\")\n",
    "            print(f\"using augmentation type: {aug_type}\")\n",
    "            print(50*\"=\")\n",
    "            # Set up the transforms\n",
    "            clip_transform, augment_transform = setup_clip_transforms()\n",
    "\n",
    "            aug_dir = os.path.join(output_dir, f\"{aug_type}\")\n",
    "            os.makedirs(aug_dir, exist_ok=True)\n",
    "\n",
    "            print(f\"Loading 3D data from AffordanceNet...\")\n",
    "            background_results = []\n",
    "            for background_path in background_paths:\n",
    "\n",
    "                if background_path is None:\n",
    "                    background_info = \"No background\"\n",
    "                else:\n",
    "                    background_info = background_path.split(\"/\")[-1]\n",
    "\n",
    "                print(f\"\\nTraining with background: {background_info}\")\n",
    "\n",
    "                bg_dir = os.path.join(aug_dir, f\"{background_info}\")\n",
    "                os.makedirs(bg_dir, exist_ok=True)\n",
    "\n",
    "                #Train on random Shape\n",
    "                global_mIou = 0\n",
    "                shape_results = []\n",
    "                for i,idx in enumerate(val_indices):\n",
    "\n",
    "                    shape_entry = dataset[idx]\n",
    "                    shape_class = shape_entry[\"shape_class\"]\n",
    "                    affordances = shape_entry[\"affordances\"]\n",
    "                    label_dict = shape_entry[\"labels_dict\"]\n",
    "\n",
    "                    # Convert coords to tensor if not already\n",
    "                    points = shape_entry[\"coords\"]\n",
    "                    if not isinstance(points, torch.Tensor):\n",
    "                        points = torch.tensor(points, device=device)\n",
    "\n",
    "                    print(f\"Loaded {len(points)} points\")\n",
    "                    shape_subdir = os.path.join(bg_dir, f\"shape_{shape_class}\")\n",
    "                    os.makedirs(shape_subdir, exist_ok=True)\n",
    "                    shape_obj  = os.path.join(shape_subdir, f\"{shape_class}_{idx}\")\n",
    "                    os.makedirs(shape_obj, exist_ok=True)\n",
    "\n",
    "                    shape_mIOU = 0\n",
    "                    affordance_ious = {}\n",
    "                    for affordance in affordances:\n",
    "\n",
    "                        affordance_subdir = os.path.join(shape_obj, f\"affordance_{affordance}\")\n",
    "                        os.makedirs(affordance_subdir, exist_ok=True)\n",
    "\n",
    "                        # Create and encode prompt\n",
    "                        prompt = generate_affordance_prompt(shape_class, affordance, strategy=prompt_strategy)\n",
    "                        print(f\"Using prompt: {prompt}\")\n",
    "                        text_features = encode_text(clip_model, prompt, device)\n",
    "\n",
    "                        # Optimize point cloud highlighting\n",
    "                        print(\"Starting optimization...\")\n",
    "                        net = optimize_point_cloud(\n",
    "                            points=points,\n",
    "                            renderer=renderer,\n",
    "                            clip_model=clip_model,\n",
    "                            clip_transform=clip_transform,\n",
    "                            augment_transform=augment_transform,\n",
    "                            encoded_text=text_features,\n",
    "                            log_dir=affordance_subdir,\n",
    "                            background_path=background_path,\n",
    "                            **kwargs\n",
    "                        )\n",
    "\n",
    "                        #Compute IoU for *this* affordance\n",
    "                        with torch.no_grad():\n",
    "                            pred_class = net(points)  # shape [N,2]\n",
    "                            highlight_scores = pred_class[:, 0]\n",
    "\n",
    "                            gt_bin = (label_dict[affordance] > 0.0).long()\n",
    "                            bin_pred = (highlight_scores >= iou_threshold).long()\n",
    "                            iou_val = compute_mIoU(bin_pred, gt_bin)\n",
    "                            print(f\"IoU: {iou_val}\")\n",
    "                            affordance_ious[affordance] = float(iou_val)\n",
    "                            shape_mIOU += iou_val\n",
    "\n",
    "                    shape_mIOU = shape_mIOU / len(affordances)\n",
    "                    print(f\"shape mIOU: {shape_mIOU}\")\n",
    "                    global_mIou += shape_mIOU\n",
    "                    # Store results for this shape\n",
    "                    shape_results.append({\n",
    "                        \"shape_id\": idx,\n",
    "                        \"shape_class\": shape_class,\n",
    "                        \"shape_miou\": float(shape_mIOU),\n",
    "                        \"affordance_ious\": affordance_ious\n",
    "                    })\n",
    "\n",
    "                global_mIou = global_mIou / len(val_indices)\n",
    "                print(f\"global mIOU: {global_mIou}\")\n",
    "\n",
    "                # Save report for this background\n",
    "                save_experiment_report(\n",
    "                    background_info=background_info,\n",
    "                    shape_results=shape_results,\n",
    "                    global_miou=global_mIou,\n",
    "                    output_dir=output_dir\n",
    "                )\n",
    "\n",
    "                # Store results for final comparison\n",
    "                background_results.append({\n",
    "                    \"background\": background_info,\n",
    "                    \"global_miou\": global_mIou\n",
    "                })\n",
    "\n",
    "                print(50 *\"=\")\n",
    "                print(50 *\"=\")\n",
    "                print(50 *\"=\")\n",
    "\n",
    "            # Print final comparison\n",
    "            print(\"\\nFinal Results Comparison:\")\n",
    "            print(f\"\\n Augumentation Type: {aug_type}\")\n",
    "            print(\"========================\")\n",
    "            for result in background_results:\n",
    "                print(f\"Background: {result['background']:<20} mIoU: {result['global_miou']:.4f}\")\n",
    "        \n",
    "       \n",
    "    except Exception as e:\n",
    "        print(f\"Error in processing: {str(e)}\")\n",
    "        raise\n",
    "\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-10T20:13:45.244144Z",
     "iopub.execute_input": "2025-01-10T20:13:45.244470Z",
     "iopub.status.idle": "2025-01-10T20:13:45.267100Z",
     "shell.execute_reply.started": "2025-01-10T20:13:45.244443Z",
     "shell.execute_reply": "2025-01-10T20:13:45.266203Z"
    },
    "id": "E0SBrmlBkwib",
    "trusted": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "background_paths = [\n",
    "    None,\n",
    "    \"/kaggle/working/Affordance3DHighlighter/data/background.jpg\",\n",
    "    \"/kaggle/working/Affordance3DHighlighter/data/background2.jpg\",\n",
    "    \"/kaggle/working/Affordance3DHighlighter/data/kitchen1.jpg\",\n",
    "    \"/kaggle/working/Affordance3DHighlighter/data/kitchen2.jpg\"\n",
    "]\n",
    "main(\n",
    "    input_path=\"/kaggle/working/Affordance3DHighlighter/data/full_shape_train_data.pkl\",\n",
    "    target_classes=['Knife'],\n",
    "    target_affordances=['cut'],\n",
    "    prompt_strategy=\"basic\",\n",
    "    iou_threshold = 0.01,\n",
    "    n_views=2,\n",
    "    n_augs=3,\n",
    "    clipavg=\"view\",\n",
    "    network_depth=4,\n",
    "    network_width=256,\n",
    "    learning_rate=1e-4,\n",
    "    num_iterations=301,\n",
    "    num_points=100000,\n",
    "    device=\"cuda\",\n",
    "    output_dir=\"./output\",\n",
    "    background_paths=background_paths\n",
    ")"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-01-10T20:13:47.180271Z",
     "iopub.execute_input": "2025-01-10T20:13:47.180731Z",
     "iopub.status.idle": "2025-01-10T20:18:18.790642Z",
     "shell.execute_reply.started": "2025-01-10T20:13:47.180701Z",
     "shell.execute_reply": "2025-01-10T20:18:18.789689Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Loading AffordanceNet Dataset...\nFound 129 valid ['Bowl'] objects with all affordances ['contain']\nSetting up CLIP model...\nSetting up renderer...\nLoading 3D data from /kaggle/working/Affordance3DHighlighter/data/full_shape_train_data.pkl...\n\nTraining with background: No background\nLoaded 2048 points\nUsing prompt: A 3D render of a gray Bowl emphasizing the main storage compartment, internal volume, and any additional pockets or compartments designed to hold and organize items\nStarting optimization...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "  1%|          | 2/200 [00:00<00:15, 12.64it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Iteration 0, Loss: -0.2401\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": " 52%|█████▏    | 104/200 [00:06<00:05, 16.54it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Iteration 100, Loss: -0.2917\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 200/200 [00:11<00:00, 16.84it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "IoU: 0.62451171875\nshape mIOU: 0.62451171875\nLoaded 2048 points\nUsing prompt: A 3D render of a gray Bowl emphasizing the main storage compartment, internal volume, and any additional pockets or compartments designed to hold and organize items\nStarting optimization...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "  1%|          | 2/200 [00:00<00:11, 16.64it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Iteration 0, Loss: -0.2194\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": " 52%|█████▏    | 104/200 [00:06<00:05, 17.14it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Iteration 100, Loss: -0.2542\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 200/200 [00:11<00:00, 17.07it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "IoU: 0.6435546875\nshape mIOU: 0.6435546875\nLoaded 2048 points\nUsing prompt: A 3D render of a gray Bowl emphasizing the main storage compartment, internal volume, and any additional pockets or compartments designed to hold and organize items\nStarting optimization...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "  1%|          | 2/200 [00:00<00:11, 16.66it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Iteration 0, Loss: -0.2362\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": " 52%|█████▏    | 104/200 [00:06<00:05, 16.96it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Iteration 100, Loss: -0.2671\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 200/200 [00:11<00:00, 16.92it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "IoU: 0.70361328125\nshape mIOU: 0.70361328125\nglobal mIOU: 0.6572265625\n==================================================\n==================================================\n==================================================\n\nTraining with background: background.jpg\nLoaded 2048 points\nUsing prompt: A 3D render of a gray Bowl emphasizing the main storage compartment, internal volume, and any additional pockets or compartments designed to hold and organize items\nStarting optimization...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "  1%|          | 2/200 [00:00<00:15, 12.74it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Iteration 0, Loss: -0.1842\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": " 51%|█████     | 102/200 [00:07<00:07, 12.88it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Iteration 100, Loss: -0.2407\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 200/200 [00:15<00:00, 13.01it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "IoU: 0.935546875\nshape mIOU: 0.935546875\nLoaded 2048 points\nUsing prompt: A 3D render of a gray Bowl emphasizing the main storage compartment, internal volume, and any additional pockets or compartments designed to hold and organize items\nStarting optimization...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "  1%|          | 2/200 [00:00<00:15, 12.98it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Iteration 0, Loss: -0.2012\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": " 51%|█████     | 102/200 [00:07<00:07, 13.08it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Iteration 100, Loss: -0.2279\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 200/200 [00:15<00:00, 12.91it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "IoU: 0.623046875\nshape mIOU: 0.623046875\nLoaded 2048 points\nUsing prompt: A 3D render of a gray Bowl emphasizing the main storage compartment, internal volume, and any additional pockets or compartments designed to hold and organize items\nStarting optimization...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "  1%|          | 2/200 [00:00<00:17, 11.58it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Iteration 0, Loss: -0.2195\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": " 51%|█████     | 102/200 [00:07<00:07, 12.82it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Iteration 100, Loss: -0.2281\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 200/200 [00:15<00:00, 12.96it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "IoU: 0.91162109375\nshape mIOU: 0.91162109375\nglobal mIOU: 0.8234049479166666\n==================================================\n==================================================\n==================================================\n\nTraining with background: background2.jpg\nLoaded 2048 points\nUsing prompt: A 3D render of a gray Bowl emphasizing the main storage compartment, internal volume, and any additional pockets or compartments designed to hold and organize items\nStarting optimization...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "  0%|          | 1/200 [00:00<00:55,  3.61it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Iteration 0, Loss: -0.1172\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": " 50%|█████     | 101/200 [00:29<00:29,  3.38it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Iteration 100, Loss: -0.1902\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 200/200 [00:58<00:00,  3.40it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "IoU: 0.60986328125\nshape mIOU: 0.60986328125\nLoaded 2048 points\nUsing prompt: A 3D render of a gray Bowl emphasizing the main storage compartment, internal volume, and any additional pockets or compartments designed to hold and organize items\nStarting optimization...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "  0%|          | 1/200 [00:00<00:59,  3.33it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Iteration 0, Loss: -0.2059\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": " 50%|█████     | 101/200 [00:29<00:29,  3.37it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Iteration 100, Loss: -0.2383\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 200/200 [00:59<00:00,  3.39it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "IoU: 0.74169921875\nshape mIOU: 0.74169921875\nLoaded 2048 points\nUsing prompt: A 3D render of a gray Bowl emphasizing the main storage compartment, internal volume, and any additional pockets or compartments designed to hold and organize items\nStarting optimization...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "  0%|          | 1/200 [00:00<00:59,  3.36it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Iteration 0, Loss: -0.2155\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": " 50%|█████     | 101/200 [00:29<00:29,  3.35it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Iteration 100, Loss: -0.2212\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 200/200 [00:58<00:00,  3.41it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "IoU: 0.6962890625\nshape mIOU: 0.6962890625\nglobal mIOU: 0.6826171875\n==================================================\n==================================================\n==================================================\n\nFinal Results Comparison:\n========================\nBackground: No background        mIoU: 0.6572\nBackground: background.jpg       mIoU: 0.8234\nBackground: background2.jpg      mIoU: 0.6826\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 47
  }
 ]
}
